{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected NN on bottleneck features\n",
    "Concatenate all preprocessed features from Inception, Xception, and InceptionResNet.\n",
    "\n",
    "Then train a fully connected NN with 1 hidden layer, last layer with softmax.\n",
    "\n",
    "Use 5 fold crossvalidation and model ensemble to produce the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import _pickle as pickle\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(session)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all train images: 10222\n",
      "Train data has 120 classes.\n",
      "Number of all test images: 10357\n",
      "ys shape: (10222, 121)\n"
     ]
    }
   ],
   "source": [
    "# Load labels and split to train/val\n",
    "NUM_CLASSES = 120\n",
    "SEED = 1993\n",
    "NFOLDS = 5\n",
    "np.random.seed(seed=SEED)\n",
    "data_dir = '../data'\n",
    "\n",
    "labels = pd.read_csv(join(data_dir, 'labels.csv'))\n",
    "print('Number of all train images: {}'.format(len(labels)))\n",
    "print(\"Train data has {} classes.\".format(len(labels.groupby('breed').count())))\n",
    "assert len(labels.groupby('breed').count()) == NUM_CLASSES, 'Number of classes in training set is not 120!'\n",
    "\n",
    "sample_submission = pd.read_csv(join(data_dir, 'sample_submission.csv'))\n",
    "print('Number of all test images: {}'.format(len(sample_submission)))\n",
    "\n",
    "# Add a 'fold' column that contains numbers from 0 to NFOLDS\n",
    "# Use it to split train/val during crossvalidation.\n",
    "labels['fold'] = pd.Series(np.random.randint(0,NFOLDS,size=(labels.shape[0])))\n",
    "labels_index = {label:i for i,label in enumerate(np.sort(np.unique(labels.breed)))}\n",
    "labels_temp = [labels_index[label] for label in labels.breed]\n",
    "ys = np.concatenate( (to_categorical(labels_temp ,num_classes=120), labels.as_matrix(columns=['fold'])), axis=1)\n",
    "print('ys shape: {}'.format(ys.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data//train//xs_bf_inception_v3\n",
      "xs_bf_inception shape: (10222, 2049) size: 20,944,878\n",
      "Loading from ../data//train//xs_bf_xception\n",
      "xs_bf_xception shape: (10222, 2049) size: 20,944,878\n",
      "Loading from ../data//train//xs_bf_irn\n",
      "xs_bf_irn shape: (10222, 1537) size: 15,711,214\n",
      "Concatenated features xs_bf shape: (10222, 5635) size: 57,600,970\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "filename = data_dir + '//train//xs_bf_inception_v3'\n",
    "print('Loading from {}'.format(filename))\n",
    "with open(filename, 'rb') as fp:\n",
    "    xs_bf_inception = pickle.load(fp)\n",
    "print('xs_bf_inception shape: {} size: {:,}'.format(xs_bf_inception.shape, xs_bf_inception.size))\n",
    "\n",
    "filename = data_dir + '//train//xs_bf_xception'\n",
    "print('Loading from {}'.format(filename))\n",
    "with open(filename, 'rb') as fp:\n",
    "    xs_bf_xception = pickle.load(fp)\n",
    "print('xs_bf_xception shape: {} size: {:,}'.format(xs_bf_xception.shape, xs_bf_xception.size))\n",
    "\n",
    "filename = data_dir + '//train//xs_bf_irn'\n",
    "print('Loading from {}'.format(filename))\n",
    "with open(filename, 'rb') as fp:\n",
    "    xs_bf_irn = pickle.load(fp)\n",
    "print('xs_bf_irn shape: {} size: {:,}'.format(xs_bf_irn.shape, xs_bf_irn.size))\n",
    "\n",
    "N_FEATURES = xs_bf_inception.shape[1] + xs_bf_xception.shape[1] + xs_bf_irn.shape[1] - 3\n",
    "\n",
    "xs_bf = pd.DataFrame(data=np.concatenate(\n",
    "    (labels.as_matrix(columns=['id', 'fold']),\n",
    "     xs_bf_inception.as_matrix(columns=xs_bf_inception.columns[0:]),\n",
    "     xs_bf_xception.as_matrix(columns=xs_bf_xception.columns[1:]),\n",
    "     xs_bf_irn.as_matrix(columns=xs_bf_irn.columns[1:])),\n",
    "     axis=1), columns=['id', 'fold', 'id2']+list(range(N_FEATURES)))\n",
    "print('Concatenated features xs_bf shape: {} size: {:,}'.format(xs_bf.shape, xs_bf.size))\n",
    "\n",
    "assert xs_bf.id.equals(xs_bf.id2), 'ID from labels has to be equal to ID from xs!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data//test//xs_bf_inception\n",
      "xs_bf_inception shape: (10357, 2049) size: 21,221,493\n",
      "Loading from ../data//test//xs_bf_xception\n",
      "xs_bf_xception shape: (10357, 2049) size: 21,221,493\n",
      "Loading from ../data//test//xs_bf_irn\n",
      "xs_bf_irn shape: (10357, 1537) size: 15,918,709\n",
      "Concatenated features xs_test_bf shape: (10357, 5633) size: 58,340,981\n"
     ]
    }
   ],
   "source": [
    "# Load test set data\n",
    "filename = data_dir + '//test//xs_bf_inception'\n",
    "print('Loading from {}'.format(filename))\n",
    "with open(filename, 'rb') as fp:\n",
    "    xs_bf_inception = pickle.load(fp)\n",
    "print('xs_bf_inception shape: {} size: {:,}'.format(xs_bf_inception.shape, xs_bf_inception.size))\n",
    "\n",
    "filename = data_dir + '//test//xs_bf_xception'\n",
    "print('Loading from {}'.format(filename))\n",
    "with open(filename, 'rb') as fp:\n",
    "    xs_bf_xception = pickle.load(fp)\n",
    "print('xs_bf_xception shape: {} size: {:,}'.format(xs_bf_xception.shape, xs_bf_xception.size))\n",
    "\n",
    "filename = data_dir + '//test//xs_bf_irn'\n",
    "print('Loading from {}'.format(filename))\n",
    "with open(filename, 'rb') as fp:\n",
    "    xs_bf_irn = pickle.load(fp)\n",
    "print('xs_bf_irn shape: {} size: {:,}'.format(xs_bf_irn.shape, xs_bf_irn.size))\n",
    "\n",
    "xs_test_bf = pd.DataFrame(data=np.concatenate(\n",
    "    (xs_bf_inception.as_matrix(columns=xs_bf_inception.columns[0:]),\n",
    "     xs_bf_xception.as_matrix(columns=xs_bf_xception.columns[1:]),\n",
    "     xs_bf_irn.as_matrix(columns=xs_bf_irn.columns[1:])\n",
    "    ), axis=1), columns=['id']+list(range(N_FEATURES)))\n",
    "print('Concatenated features xs_test_bf shape: {} size: {:,}'.format(xs_test_bf.shape, xs_test_bf.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(200, input_shape=(N_FEATURES,), kernel_regularizer=l2(0.00001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(120, activation='softmax', kernel_regularizer=l2(0.0000001)))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### fold 0 ###\n",
      "Training data x: (8138, 5632) y: (8138, 120)\n",
      "Validation data x: (2084, 5632) y: (2084, 120)\n",
      "2084/2084 [==============================] - 0s 225us/step\n",
      "Validation loss: 0.281237\n",
      "Validation accuracy: 91.891%\n",
      "Predicted ys_train[0] shape: (10222, 120) size: 1,226,640\n",
      "10357/10357 [==============================] - 1s 84us/step\n",
      "Predicted ys_test[0] shape: (10357, 120) size: 1,242,840\n",
      "\n",
      "### fold 1 ###\n",
      "Training data x: (8213, 5632) y: (8213, 120)\n",
      "Validation data x: (2009, 5632) y: (2009, 120)\n",
      "2009/2009 [==============================] - 1s 264us/step\n",
      "Validation loss: 0.289977\n",
      "Validation accuracy: 91.538%\n",
      "Predicted ys_train[1] shape: (10222, 120) size: 1,226,640\n",
      "10357/10357 [==============================] - 1s 83us/step\n",
      "Predicted ys_test[1] shape: (10357, 120) size: 1,242,840\n",
      "\n",
      "### fold 2 ###\n",
      "Training data x: (8118, 5632) y: (8118, 120)\n",
      "Validation data x: (2104, 5632) y: (2104, 120)\n",
      "2104/2104 [==============================] - 1s 238us/step\n",
      "Validation loss: 0.245679\n",
      "Validation accuracy: 92.728%\n",
      "Predicted ys_train[2] shape: (10222, 120) size: 1,226,640\n",
      "10357/10357 [==============================] - 1s 84us/step\n",
      "Predicted ys_test[2] shape: (10357, 120) size: 1,242,840\n",
      "\n",
      "### fold 3 ###\n",
      "Training data x: (8138, 5632) y: (8138, 120)\n",
      "Validation data x: (2084, 5632) y: (2084, 120)\n",
      "2084/2084 [==============================] - 0s 225us/step\n",
      "Validation loss: 0.250923\n",
      "Validation accuracy: 92.131%\n",
      "Predicted ys_train[3] shape: (10222, 120) size: 1,226,640\n",
      "10357/10357 [==============================] - 1s 83us/step\n",
      "Predicted ys_test[3] shape: (10357, 120) size: 1,242,840\n",
      "\n",
      "### fold 4 ###\n",
      "Training data x: (8281, 5632) y: (8281, 120)\n",
      "Validation data x: (1941, 5632) y: (1941, 120)\n",
      "1941/1941 [==============================] - 0s 201us/step\n",
      "Validation loss: 0.244601\n",
      "Validation accuracy: 92.530%\n",
      "Predicted ys_train[4] shape: (10222, 120) size: 1,226,640\n",
      "10357/10357 [==============================] - 1s 81us/step\n",
      "Predicted ys_test[4] shape: (10357, 120) size: 1,242,840\n",
      "\n",
      "Fertig\n"
     ]
    }
   ],
   "source": [
    "models = [{} for fold in range(NFOLDS)]\n",
    "histories = [{} for fold in range(NFOLDS)]\n",
    "scores = [{} for fold in range(NFOLDS)]\n",
    "ys_train = np.zeros((NFOLDS, xs_bf.shape[0], NUM_CLASSES))\n",
    "ys_test = np.zeros((NFOLDS, xs_test_bf.shape[0], NUM_CLASSES))\n",
    "\n",
    "for fold in range(NFOLDS):\n",
    "    print('\\n### fold {} ###'.format(fold))\n",
    "    \n",
    "    # Split data\n",
    "    x_tr = xs_bf[xs_bf.fold != fold].as_matrix(columns=xs_bf.columns[3:]).astype('float32')\n",
    "    y_tr = ys[ys[:,-1] != fold][:,:-1].astype('float32')\n",
    "    print('Training data x: {} y: {}'.format(x_tr.shape, y_tr.shape))\n",
    "    x_val = xs_bf[xs_bf.fold == fold].as_matrix(columns=xs_bf.columns[3:]).astype('float32')\n",
    "    y_val = ys[ys[:,-1] == fold][:,:-1].astype('float32')\n",
    "    print('Validation data x: {} y: {}'.format(x_val.shape, y_val.shape))\n",
    "    \n",
    "    # Setup model\n",
    "    models[fold] = setupModel()\n",
    "    # models[fold].summary()\n",
    "    \n",
    "    # Fit the model\n",
    "    histories[fold] = models[fold].fit(x_tr, y_tr,\n",
    "          batch_size=1500,\n",
    "          epochs=50,\n",
    "          verbose=0,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=2)])\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores[fold] = models[fold].evaluate(x_val, y_val, verbose=1)\n",
    "    print('Validation loss: {:.6f}'.format(scores[fold][0]))\n",
    "    print('Validation accuracy: {:.3%}'.format(scores[fold][1]))\n",
    "    \n",
    "    # Predict on train set\n",
    "    ys_train[fold] = models[fold].predict(\n",
    "        xs_bf.as_matrix(columns=xs_bf.columns[3:]).astype('float32'),\n",
    "        batch_size=512,\n",
    "        verbose=0)\n",
    "    print('Predicted ys_train[{}] shape: {} size: {:,}'.format(fold, ys_train[fold].shape, ys_train[fold].size))\n",
    "\n",
    "    # Predict on test set\n",
    "    ys_test[fold] = models[fold].predict(\n",
    "        xs_test_bf.as_matrix(columns=xs_test_bf.columns[1:]).astype('float32'),\n",
    "        batch_size=512,\n",
    "        verbose=1)\n",
    "    print('Predicted ys_test[{}] shape: {} size: {:,}'.format(fold, ys_test[fold].shape, ys_test[fold].size))\n",
    "    \n",
    "print('\\nFertig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss and accuracy\n",
      "[[ 0.28123696  0.91890595]\n",
      " [ 0.28997732  0.91538079]\n",
      " [ 0.24567856  0.92728137]\n",
      " [ 0.25092281  0.92130518]\n",
      " [ 0.24460087  0.92529624]]\n",
      "Prediction shape: (10222, 120)\n",
      "Validation LogLoss 0.14085716095582876\n",
      "Validation Accuracy 0.9551946781451771\n"
     ]
    }
   ],
   "source": [
    "print('Average loss and accuracy')\n",
    "print(np.asarray(scores) )\n",
    "\n",
    "# Only pick the best models\n",
    "w = [0, 0, 1, 1 ,1]\n",
    "\n",
    "# Average over the best models and evaluate on the whole train set\n",
    "train_predictions = np.average(ys_train, axis=0, weights=w)\n",
    "print('Prediction shape: {}'.format(train_predictions.shape))\n",
    "\n",
    "print('Validation LogLoss {}'.format(log_loss( ys[:,:-1], train_predictions)))\n",
    "print('Validation Accuracy {}'.format(accuracy_score(\n",
    "    (ys[:,:-1] * range(NUM_CLASSES)).sum(axis=1),\n",
    "    np.argmax(train_predictions, axis=1) )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (10357, 120)\n"
     ]
    }
   ],
   "source": [
    "# Average over all models for test set\n",
    "predictions = np.average(ys_test, axis=0, weights=w)\n",
    "print('Prediction shape: {}'.format(predictions.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "for i, breed in enumerate(sample_submission.columns[1:]):\n",
    "    sample_submission[breed] = predictions[:, labels_index[breed]]\n",
    "    \n",
    "sample_submission.to_csv('..//submissions//sub_crossval_nn_inc_xc_irn_l2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
