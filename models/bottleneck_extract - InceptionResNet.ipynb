{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract InceptionResNet bottleneck features\n",
    "From raw image, get 1536 features of the penultimate InceptionResNet layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import _pickle as pickle\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(session)\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical, plot_model\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(img_id, train_or_test, size=None):\n",
    "    img = image.load_img(join(data_dir, train_or_test, '%s.jpg' % img_id), target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all train images: 10222\n",
      "Train data has 120 classes.\n",
      "Number of all test images: 10357\n",
      "y_tr shape: (8185, 120)\n",
      "y_val shape: (2037, 120)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 120\n",
    "SEED = 1993\n",
    "np.random.seed(seed=SEED)\n",
    "data_dir = '../data'\n",
    "\n",
    "labels = pd.read_csv(join(data_dir, 'labels.csv'))\n",
    "print('Number of all train images: {}'.format(len(labels)))\n",
    "print(\"Train data has {} classes.\".format(len(labels.groupby('breed').count())))\n",
    "assert len(labels.groupby('breed').count()) == NUM_CLASSES, 'Number of classes in training set is not 120!'\n",
    "\n",
    "sample_submission = pd.read_csv(join(data_dir, 'sample_submission.csv'))\n",
    "print('Number of all test images: {}'.format(len(sample_submission)))\n",
    "\n",
    "# Split to train and validation sets\n",
    "l_val = labels.groupby('breed').apply(pd.DataFrame.sample, frac=0.2).reset_index(drop=True)\n",
    "l_tr = labels.loc[~labels['id'].isin(l_val['id'])]\n",
    "l_tr_index = {label:i for i,label in enumerate(np.unique(l_tr.breed))}\n",
    "l_tr_temp = [l_tr_index[label] for label in l_tr.breed]\n",
    "l_val_temp = [l_tr_index[label] for label in l_val.breed]\n",
    "y_tr = to_categorical(l_tr_temp ,num_classes=120)\n",
    "y_val = to_categorical(l_val_temp ,num_classes=120)\n",
    "print('y_tr shape: {}'.format(y_tr.shape))\n",
    "print('y_val shape: {}'.format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from raw images\n",
      "All train Images shape: (10222, 299, 299, 3) size: 2,741,571,066\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 47s 0us/step\n",
      "10222/10222 [==============================] - 1786s 175ms/step\n",
      "All train bottleneck features shape: (10222, 1536) size: 15,700,992\n",
      "Dumping into ../data//train//xs_bf_irs\n",
      "Train bottleneck features shape: (8185, 1536) size: 12,572,160\n",
      "Validation bottleneck features shape: (2037, 1536) size: 3,128,832\n"
     ]
    }
   ],
   "source": [
    "# Get bottelneck features\n",
    "INPUT_SIZE = 299\n",
    "POOLING = 'avg'\n",
    "filename = data_dir + '//train//xs_bf_irn'\n",
    "\n",
    "if isfile(filename):\n",
    "    # Load from file\n",
    "    print('Loading from {}'.format(filename))\n",
    "    with open(filename, 'rb') as fp:\n",
    "        xs_bf = pickle.load(fp)\n",
    "    print('xs_bf shape: {} size: {:,}'.format(xs_bf.shape, xs_bf.size))\n",
    "else:\n",
    "    # Preprocess images\n",
    "    print('Reading from raw images')\n",
    "    xs = np.zeros((len(labels), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "    for i, img_id in enumerate(labels['id']):\n",
    "        img = read_img(img_id, 'train', (INPUT_SIZE, INPUT_SIZE))\n",
    "        x = preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "        xs[i] = x\n",
    "    print('All train Images shape: {} size: {:,}'.format(xs.shape, xs.size))\n",
    "    \n",
    "    # Predict\n",
    "    preprocess_bottleneck = InceptionResNetV2(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "    xs_bf_raw = preprocess_bottleneck.predict(xs, batch_size=6, verbose=1)\n",
    "    print('All train bottleneck features shape: {} size: {:,}'.format(xs_bf_raw.shape, xs_bf_raw.size))\n",
    "    xs_bf = pd.concat([labels.id, pd.DataFrame(xs_bf_raw)], axis=1)\n",
    "    \n",
    "    # Save into file\n",
    "    print('Dumping into {}'.format(filename))\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(xs_bf, fp)\n",
    "\n",
    "# Split to train/val sets\n",
    "x_tr = np.zeros((len(l_tr), xs_bf.shape[1]-1), dtype='float32')\n",
    "for i, img_id in enumerate(l_tr['id']):\n",
    "    x_tr[i] = xs_bf[xs_bf.id == img_id].values[0][1:]\n",
    "print('Train bottleneck features shape: {} size: {:,}'.format(x_tr.shape, x_tr.size))\n",
    "\n",
    "x_val = np.zeros((len(l_val), xs_bf.shape[1]-1), dtype='float32')\n",
    "for i, img_id in enumerate(l_val['id']):\n",
    "    x_val[i] = xs_bf[xs_bf.id == img_id].values[0][1:]\n",
    "print('Validation bottleneck features shape: {} size: {:,}'.format(x_val.shape, x_val.size))\n",
    "\n",
    "xs = None\n",
    "xs_bf = None\n",
    "xs_bf_raw = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation LogLoss 0.31869074149026316\n",
      "Validation Accuracy 0.9072164948453608\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on bottleneck features\n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n",
    "logreg.fit(x_tr, (y_tr * range(NUM_CLASSES)).sum(axis=1))\n",
    "valid_probs = logreg.predict_proba(x_val)\n",
    "valid_preds = logreg.predict(x_val)\n",
    "print('Validation LogLoss {}'.format(log_loss(y_val, valid_probs)))\n",
    "print('Validation Accuracy {}'.format(accuracy_score((y_val * range(NUM_CLASSES)).sum(axis=1), valid_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from raw images\n",
      "All test Images shape: (10357, 299, 299, 3) size: 2,777,778,471\n",
      "10357/10357 [==============================] - 1791s 173ms/step\n",
      "All test bottleneck features shape: (10357, 1536) size: 15,908,352\n",
      "Dumping into ../data//test//xs_bf_irs\n"
     ]
    }
   ],
   "source": [
    "# Get test bottelneck features\n",
    "INPUT_SIZE = 299\n",
    "POOLING = 'avg'\n",
    "filename = data_dir + '//test//xs_bf_irn'\n",
    "\n",
    "if isfile(filename):\n",
    "    print('File {} already exists!'.format(filename))\n",
    "else:\n",
    "    # Preprocess images\n",
    "    print('Reading from raw images')\n",
    "    xs = np.zeros((len(sample_submission.id), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "    for i, img_id in enumerate(sample_submission['id']):\n",
    "        img = read_img(img_id, 'test', (INPUT_SIZE, INPUT_SIZE))\n",
    "        x = preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "        xs[i] = x\n",
    "    print('All test Images shape: {} size: {:,}'.format(xs.shape, xs.size))\n",
    "    \n",
    "    # Predict\n",
    "    preprocess_bottleneck = InceptionResNetV2(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "    xs_bf_raw = preprocess_bottleneck.predict(xs, batch_size=6, verbose=1)\n",
    "    print('All test bottleneck features shape: {} size: {:,}'.format(xs_bf_raw.shape, xs_bf_raw.size))\n",
    "    xs_bf = pd.concat([sample_submission.id, pd.DataFrame(xs_bf_raw)], axis=1)\n",
    "    \n",
    "    # Save into file\n",
    "    print('Dumping into {}'.format(filename))\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(xs_bf, fp)\n",
    "\n",
    "xs = None\n",
    "xs_bf = None\n",
    "xs_bf_raw = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
