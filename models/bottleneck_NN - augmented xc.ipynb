{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected NN on bottleneck features from augmented images\n",
    "Input is the original plus augmentation features from Xception.\n",
    "\n",
    "Then train a fully connected NN with 1 hidden layer, last layer with softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import _pickle as pickle\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(session)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_augmented_labels(ls, inds):\n",
    "    temp_augs = np.tile(np.arange(NUM_AUGS), (len(inds), 1))\n",
    "    temp_inds = inds.values.reshape((len(inds), 1)) * NUM_AUGS\n",
    "    new_inds = np.add(temp_inds, temp_augs).reshape(len(inds)*NUM_AUGS)\n",
    "    new_ls = ls.reindex(np.repeat(ls.index.values, NUM_AUGS), method='ffill')\n",
    "    new_ls = new_ls.iloc[new_inds, :]\n",
    "    return new_ls, new_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all train images: 10222\n",
      "Train data has 120 classes.\n",
      "Number of all test images: 10357\n",
      "y_tr shape: (81850, 120)\n",
      "y_val shape: (20370, 120)\n"
     ]
    }
   ],
   "source": [
    "# Load labels and split to train/val\n",
    "NUM_CLASSES = 120\n",
    "SEED = 1993\n",
    "NUM_AUGS = 10\n",
    "np.random.seed(seed=SEED)\n",
    "data_dir = '../data'\n",
    "\n",
    "labels = pd.read_csv(join(data_dir, 'labels.csv'))\n",
    "print('Number of all train images: {}'.format(len(labels)))\n",
    "print(\"Train data has {} classes.\".format(len(labels.groupby('breed').count())))\n",
    "assert len(labels.groupby('breed').count()) == NUM_CLASSES, 'Number of classes in training set is not 120!'\n",
    "\n",
    "sample_submission = pd.read_csv(join(data_dir, 'sample_submission.csv'))\n",
    "print('Number of all test images: {}'.format(len(sample_submission)))\n",
    "\n",
    "# Split to train and validation sets\n",
    "temp_labels_tr = labels.groupby('breed').apply(pd.DataFrame.sample, frac=0.8)\n",
    "temp_labels_val = labels.loc[~labels['id'].isin(temp_labels_tr['id'])]\n",
    "l_tr, inds_tr = add_augmented_labels(labels, temp_labels_tr.index.levels[1])\n",
    "l_val, inds_val = add_augmented_labels(labels, temp_labels_val.index)\n",
    "\n",
    "breed_index = {label:i for i,label in enumerate(np.unique(l_tr.breed))}\n",
    "l_tr_temp = [breed_index[label] for label in l_tr.breed]\n",
    "l_val_temp = [breed_index[label] for label in l_val.breed]\n",
    "y_tr = to_categorical(l_tr_temp ,num_classes=120)\n",
    "y_val = to_categorical(l_val_temp ,num_classes=120)\n",
    "\n",
    "print('y_tr shape: {}'.format(y_tr.shape))\n",
    "print('y_val shape: {}'.format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data//train//xs_bf_xception_aug\n",
      "xs_bf shape: (102220, 2049) size: 209,448,780\n"
     ]
    }
   ],
   "source": [
    "filename = data_dir + '//train//xs_bf_xception_aug'\n",
    "print('Loading from {}'.format(filename))\n",
    "with open(filename, 'rb') as fp:\n",
    "    xs_bf = pickle.load(fp)\n",
    "print('xs_bf shape: {} size: {:,}'.format(xs_bf.shape, xs_bf.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bottleneck features shape: (81850, 2048) size: 167,628,800\n",
      "Validation bottleneck features shape: (20370, 2048) size: 41,717,760\n"
     ]
    }
   ],
   "source": [
    "# Split to train/val sets\n",
    "x_tr = xs_bf.values[inds_tr, :-1]\n",
    "print('Train bottleneck features shape: {} size: {:,}'.format(x_tr.shape, x_tr.size))\n",
    "\n",
    "x_val = xs_bf.values[inds_val, :-1]\n",
    "print('Validation bottleneck features shape: {} size: {:,}'.format(x_val.shape, x_val.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train bottleneck features shape: (8185, 2048) size: 16,762,880\n"
     ]
    }
   ],
   "source": [
    "# Only original images\n",
    "x_tr_orig = x_tr[::10]\n",
    "y_tr_orig = y_tr[::10]\n",
    "x_val_orig = x_val[::10]\n",
    "y_val_orig = y_val[::10]\n",
    "print('Original train bottleneck features shape: {} size: {:,}'.format(x_tr_orig.shape, x_tr_orig.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balanced train bottleneck features shape: (12121, 2048) size: 24,823,808\n"
     ]
    }
   ],
   "source": [
    "# Balance classes with augmented examples such that each class has exactly 101 images\n",
    "max_class_n = max(y_tr_orig.sum(axis=0)) # 101\n",
    "topick = [4, 5, 8, 9]\n",
    "inds_class = []\n",
    "for i in range(120):\n",
    "    current_class_n = y_tr_orig.sum(axis=0)[i]\n",
    "    for j in range(y_tr_orig.shape[0]):\n",
    "        if np.argmax(y_tr_orig[j]) == i:\n",
    "            current_class_n = current_class_n + 1\n",
    "            inds_class.append(10*j + 1)\n",
    "            if max_class_n <= current_class_n:\n",
    "                break\n",
    "x_tr_class = np.concatenate((x_tr_orig, x_tr[inds_class]))\n",
    "y_tr_class = np.concatenate((y_tr_orig, y_tr[inds_class]))\n",
    "print('Class balanced train bottleneck features shape: {} size: {:,}'.format(x_tr_class.shape, x_tr_class.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037/2037 [==============================] - 1s 307us/step\n",
      "Train on original images validation loss: 0.309 | acc: 90.9%\n",
      "2037/2037 [==============================] - 0s 238us/step\n",
      "Train on class balanced images validation loss: 0.316 | acc: 89.3%\n",
      "2037/2037 [==============================] - 1s 269us/step\n",
      "Train on all images validation loss: 0.320 | acc: 90.2%\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "\n",
    "# Save predictions on all images into preds variable\n",
    "preds = []\n",
    "\n",
    "def setup_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_shape=(2048,), kernel_regularizer=l2(0.00001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(120, activation='softmax', kernel_regularizer=l2(0.000001)))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# Train on original images\n",
    "model = setup_model()\n",
    "model.fit(x_tr_orig, y_tr_orig,\n",
    "          batch_size=512,\n",
    "          epochs=50,\n",
    "          verbose=0,\n",
    "          validation_data=(x_val_orig, y_val_orig),\n",
    "          shuffle=True,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=2)])\n",
    "scores = model.evaluate(x_val_orig, y_val_orig)\n",
    "print('Train on original images validation loss: {:.3f} | acc: {:.1%}'.format(scores[0], scores[1]))\n",
    "preds.append(model.predict(x_val, batch_size=512))\n",
    "\n",
    "# Train on class balanced images\n",
    "model = setup_model()\n",
    "model.fit(x_tr_class, y_tr_class,\n",
    "          batch_size=512,\n",
    "          epochs=50,\n",
    "          verbose=0,\n",
    "          validation_data=(x_val_orig, y_val_orig),\n",
    "          shuffle=True,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=2)])\n",
    "scores = model.evaluate(x_val_orig, y_val_orig)\n",
    "print('Train on class balanced images validation loss: {:.3f} | acc: {:.1%}'.format(scores[0], scores[1]))\n",
    "preds.append(model.predict(x_val, batch_size=512))\n",
    "\n",
    "# Train on all images\n",
    "model = setup_model()\n",
    "model.fit(x_tr, y_tr,\n",
    "          batch_size=5120,\n",
    "          epochs=50,\n",
    "          verbose=0,\n",
    "          validation_data=(x_val_orig, y_val_orig),\n",
    "          shuffle=True,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=2)])\n",
    "scores = model.evaluate(x_val_orig, y_val_orig)\n",
    "print('Train on all images validation loss: {:.3f} | acc: {:.1%}'.format(scores[0], scores[1]))\n",
    "preds.append(model.predict(x_val, batch_size=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and accuracy for all models on all images\n",
    "loss = np.zeros((3, 4))\n",
    "acc = np.zeros((3, 4))\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    \n",
    "    pred_all = pred\n",
    "    loss[i][0] = log_loss(y_val, pred_all)\n",
    "    acc[i][0] = accuracy_score((y_val * range(NUM_CLASSES)).sum(axis=1), np.argmax(pred_all, axis=1))\n",
    "\n",
    "    pred_orig = pred[::10]\n",
    "    loss[i][1] = log_loss(y_val_orig, pred_orig)\n",
    "    acc[i][1] = accuracy_score((y_val_orig * range(NUM_CLASSES)).sum(axis=1), np.argmax(pred_orig, axis=1))\n",
    "    \n",
    "    pred_avg = np.average(pred.reshape((int(pred.shape[0]/10), 10, 120)), axis=1)\n",
    "    pred_avg = pred_avg / np.sum(pred_avg, axis=0)\n",
    "    loss[i][2] = log_loss(y_val_orig, pred_avg)\n",
    "    acc[i][2] = accuracy_score((y_val_orig * range(NUM_CLASSES)).sum(axis=1), np.argmax(pred_avg, axis=1))\n",
    "    \n",
    "    pred_flip = np.average(pred.reshape((int(pred.shape[0]/10), 10, 120)), axis=1, weights=[0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    pred_flip = pred_flip / np.sum(pred_flip, axis=0)\n",
    "    loss[i][3] = log_loss(y_val_orig, pred_flip)\n",
    "    acc[i][3] = accuracy_score((y_val_orig * range(NUM_CLASSES)).sum(axis=1), np.argmax(pred_flip, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation categorical crossentropy loss:\n",
      "  Original | Class balanced | Augmented  \n",
      "    0.551  |      0.558     |   0.498   <- Validation on all images\n",
      "    0.300  |      0.308     |   0.311   <- Validation on original images only\n",
      "    0.384  |      0.396     |   0.381   <- Validation on all images averaged\n",
      "    0.282  |      0.291     |   0.294   <- Validation on original and flipped image, averaged\n",
      "\n",
      "Validation accuracy:\n",
      "  Original | Class balanced | Augmented  \n",
      "  83.490%  |     82.941%    |  84.831%   <- Validation on all images\n",
      "  90.869%  |     89.347%    |  90.231%   <- Validation on original images only\n",
      "  88.807%  |     89.249%    |  89.543%   <- Validation on all images averaged\n",
      "  90.967%  |     90.280%    |  90.427%   <- Validation on original and flipped image, averaged\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notes = [\n",
    "    'Validation on all images',\n",
    "    'Validation on original images only',\n",
    "    'Validation on all images averaged',\n",
    "    'Validation on original and flipped image, averaged'\n",
    "]\n",
    "\n",
    "print('Validation categorical crossentropy loss:')\n",
    "print('  Original | Class balanced | Augmented  ')\n",
    "for i in range(4):\n",
    "    print('    {:.3f}  |      {:.3f}     |   {:.3f}   <- {}'.format(loss[0][i], loss[1][i], loss[2][i], notes[i]))\n",
    "print()\n",
    "\n",
    "print('Validation accuracy:')\n",
    "print('  Original | Class balanced | Augmented  ')\n",
    "for i in range(4):\n",
    "    print('  {:.3%}  |     {:.3%}    |  {:.3%}   <- {}'.format(acc[0][i], acc[1][i], acc[2][i], notes[i]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tables above, the results of 3 models on 4 validation sets are summarized. Columns represent models, rows represent validation image sets. Note that the 1st validation set (on all images) is not very useful, as it cannot be used to make a submission.\n",
    "\n",
    "Instead of retraining the xception imagenet model weights, we only use the xception model to get the bottleneck features (2048 values of the penultimate layer). My guess is that image augmentation would be much more helpful in the former case, as literature suggests. However, in the results above, the image augmentation does not help. \n",
    "(I played with the models manually in other notebooks as well but the non-augmented model was always better)\n",
    "\n",
    "On the other hand, a pleasant discovery occured. Turns out when evaluating a model, it performs better when evaluated on the average prediction of the original and the horizontally flipped image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
