{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import _pickle as pickle\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(session)\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical, plot_model\n",
    "\n",
    "from keras.applications import xception\n",
    "from keras.applications import inception_v3\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(img_id, train_or_test, size=None):\n",
    "    img = image.load_img(join(data_dir, train_or_test, '%s.jpg' % img_id), target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10225 = 10222 = 2037 + 8185\n",
      "Test: 10359 = 10357\n",
      "We should have 120 clasess and the train data has 120 classes.\n",
      "y_tr shape: (8185, 120)\n",
      "y_val shape: (2037, 120)\n"
     ]
    }
   ],
   "source": [
    "# Load labels and split to train/val\n",
    "NUM_CLASSES = 120\n",
    "SEED = 1993\n",
    "np.random.seed(seed=SEED)\n",
    "data_dir = '../data'\n",
    "labels = pd.read_csv(join(data_dir, 'labels.csv'))\n",
    "l_val = labels.groupby('breed').apply(pd.DataFrame.sample, frac=0.2).reset_index(drop=True)\n",
    "l_tr = labels.loc[~labels['id'].isin(l_val['id'])]\n",
    "sample_submission = pd.read_csv(join(data_dir, 'sample_submission.csv'))\n",
    "print('Train:', len(listdir(join(data_dir, 'train'))), '=', len(l_tr)+len(l_val), '=', len(l_val), '+', len(l_tr))\n",
    "print('Test:', len(listdir(join(data_dir, 'test'))), '=', len(sample_submission))\n",
    "\n",
    "print(\"We should have {} clasess and the train data has {} classes.\"\n",
    "      .format(NUM_CLASSES, len(l_tr.groupby('breed').count())))\n",
    "\n",
    "l_tr_index = {label:i for i,label in enumerate(np.unique(l_tr.breed))}\n",
    "l_tr_temp = [l_tr_index[label] for label in l_tr.breed]\n",
    "l_val_temp = [l_tr_index[label] for label in l_val.breed]\n",
    "y_tr = to_categorical(l_tr_temp ,num_classes=120)\n",
    "y_val = to_categorical(l_val_temp ,num_classes=120)\n",
    "\n",
    "print('y_tr shape: {}'.format(y_tr.shape))\n",
    "print('y_val shape: {}'.format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data//train//xs_bf_inception_v3\n",
      "xs_bf shape: (10222, 2049) size: 20,944,878\n",
      "Train bottleneck features shape: (8185, 2048) size: 16,762,880\n",
      "Validation bottleneck features shape: (2037, 2048) size: 4,171,776\n"
     ]
    }
   ],
   "source": [
    "# Get bottelneck features\n",
    "INPUT_SIZE = 299\n",
    "POOLING = 'avg'\n",
    "filename = data_dir + '//train//xs_bf_inception_v3'\n",
    "\n",
    "if isfile(filename):\n",
    "    # Load from file\n",
    "    print('Loading from {}'.format(filename))\n",
    "    with open(filename, 'rb') as fp:\n",
    "        xs_bf = pickle.load(fp)\n",
    "    print('xs_bf shape: {} size: {:,}'.format(xs_bf.shape, xs_bf.size))\n",
    "else:\n",
    "    # Preprocess images\n",
    "    print('Reading from raw images')\n",
    "    xs = np.zeros((len(labels), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "    for i, img_id in enumerate(labels['id']):\n",
    "        img = read_img(img_id, 'train', (INPUT_SIZE, INPUT_SIZE))\n",
    "        x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "        xs[i] = x\n",
    "    print('All train Images shape: {} size: {:,}'.format(xs.shape, xs.size))\n",
    "    \n",
    "    # Predict\n",
    "    preprocess_bottleneck = inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "    xs_bf_raw = preprocess_bottleneck.predict(xs, batch_size=6, verbose=1)\n",
    "    print('All train bottleneck features shape: {} size: {:,}'.format(xs_bf_raw.shape, xs_bf_raw.size))\n",
    "    xs_bf = pd.concat([labels.id, pd.DataFrame(xs_bf_raw)], axis=1)\n",
    "    \n",
    "    # Save into file\n",
    "    print('Dumping into {}'.format(filename))\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(xs_bf, fp)\n",
    "\n",
    "# Split to train/val sets\n",
    "x_tr = np.zeros((len(l_tr), xs_bf.shape[1]-1), dtype='float32')\n",
    "for i, img_id in enumerate(l_tr['id']):\n",
    "    x_tr[i] = xs_bf[xs_bf.id == img_id].values[0][1:]\n",
    "print('Train bottleneck features shape: {} size: {:,}'.format(x_tr.shape, x_tr.size))\n",
    "\n",
    "x_val = np.zeros((len(l_val), xs_bf.shape[1]-1), dtype='float32')\n",
    "for i, img_id in enumerate(l_val['id']):\n",
    "    x_val[i] = xs_bf[xs_bf.id == img_id].values[0][1:]\n",
    "print('Validation bottleneck features shape: {} size: {:,}'.format(x_val.shape, x_val.size))\n",
    "\n",
    "xs = None\n",
    "xs_bf = None\n",
    "xs_bf_raw = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation LogLoss 0.3684238395832664\n",
      "Validation Accuracy 0.8880706921944035\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on bottleneck features\n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n",
    "logreg.fit(x_tr, (y_tr * range(NUM_CLASSES)).sum(axis=1))\n",
    "valid_probs = logreg.predict_proba(x_val)\n",
    "valid_preds = logreg.predict(x_val)\n",
    "print('Validation LogLoss {}'.format(log_loss(y_val, valid_probs)))\n",
    "print('Validation Accuracy {}'.format(accuracy_score((y_val * range(NUM_CLASSES)).sum(axis=1), valid_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from raw images\n",
      "All test Images shape: (10357, 299, 299, 3) size: 2,777,778,471\n",
      "10357/10357 [==============================] - 1000s  \n",
      "All test bottleneck features shape: (10357, 2048) size: 21,211,136\n",
      "Dumping into ../data//test//xs_bf_inception\n"
     ]
    }
   ],
   "source": [
    "# Get test bottelneck features\n",
    "INPUT_SIZE = 299\n",
    "POOLING = 'avg'\n",
    "filename = data_dir + '//test//xs_bf_inception'\n",
    "\n",
    "if isfile(filename):\n",
    "    print('File {} already exists!'.format(filename))\n",
    "else:\n",
    "    # Preprocess images\n",
    "    print('Reading from raw images')\n",
    "    xs = np.zeros((len(sample_submission.id), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "    for i, img_id in enumerate(sample_submission['id']):\n",
    "        img = read_img(img_id, 'test', (INPUT_SIZE, INPUT_SIZE))\n",
    "        x = inception_v3.preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "        xs[i] = x\n",
    "    print('All test Images shape: {} size: {:,}'.format(xs.shape, xs.size))\n",
    "    \n",
    "    # Predict\n",
    "    preprocess_bottleneck = inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "    xs_bf_raw = preprocess_bottleneck.predict(xs, batch_size=6, verbose=1)\n",
    "    print('All test bottleneck features shape: {} size: {:,}'.format(xs_bf_raw.shape, xs_bf_raw.size))\n",
    "    xs_bf = pd.concat([sample_submission.id, pd.DataFrame(xs_bf_raw)], axis=1)\n",
    "    \n",
    "    # Save into file\n",
    "    print('Dumping into {}'.format(filename))\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(xs_bf, fp)\n",
    "\n",
    "xs = None\n",
    "xs_bf = None\n",
    "xs_bf_raw = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
